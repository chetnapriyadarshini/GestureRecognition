{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05d41a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from imageio import imread\n",
    "from skimage.transform import resize\n",
    "import datetime\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation\n",
    "from tensorflow.keras.layers import Conv2D, Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a1bce3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37332b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('/datasets/Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('/datasets/Project_data/val.csv').readlines())\n",
    "batch_size = 100 #experiment with the batch size\n",
    "img_idx = [0,2,4,6,8,10,12,14,16,18,20,22,24,26,28]#create a list of image numbers you want to use\n",
    "num_frames = len(img_idx)\n",
    "num_classes = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c599a952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_channel(channel):\n",
    "    mean = np.mean(channel)\n",
    "    std = np.std(channel)\n",
    "    normalized_channel = (channel - mean) / std\n",
    "    return normalized_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8bfdffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(source_path, folder_list, batch_size, width, height, crop_dims):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(t)//batch_size # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,num_frames,width,height,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,num_classes)) # batch_labels is the one hot representation of the output\n",
    "            #print(batch_data.shape)\n",
    "            #print(batch_labels.shape)\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape\n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    image = crop_and_resize(image, width, height, crop_dims)\n",
    "\n",
    "                    batch_data[folder,idx,:,:,0] = normalize_channel(image[:,:,0]) #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = normalize_channel(image[:,:,1]) #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = normalize_channel(image[:,:,2]) #normalise and feed in the image\n",
    "\n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "\n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        remaining_batches = len(t)%batch_size\n",
    "        print(remaining_batches)\n",
    "        if remaining_batches > 0:\n",
    "            batch_data = np.zeros((remaining_batches,num_frames,width,height,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((remaining_batches,num_classes)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(remaining_batches): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape\n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    image = crop_and_resize(image, width, height, crop_dims)\n",
    "\n",
    "                    batch_data[folder,idx,:,:,0] = normalize_channel(image[:,:,0]) #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = normalize_channel(image[:,:,1]) #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = normalize_channel(image[:,:,2]) #normalise and feed in the image\n",
    "\n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be48cb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_resize(image, width, height,crop_dims):\n",
    "    curr_width, curr_height, channel = image.shape\n",
    "    curr_aspect_ratio = curr_width/curr_height\n",
    "    target_aspect_ratio = width/height\n",
    "    crop_dimensions = (crop_dims)\n",
    "   # print(\"curr_aspect_ratio\", curr_aspect_ratio)\n",
    "   # print(\"curr_width\", curr_width)\n",
    "   # print(\"curr_height\", curr_height)\n",
    "\n",
    "    # Calculate the cropping dimensions\n",
    "    if curr_aspect_ratio > target_aspect_ratio:\n",
    "        new_width = int(height * target_aspect_ratio)\n",
    "        offset = (width - new_width) // 2\n",
    "        cropped_image = image[:, offset:offset + new_width:]\n",
    "    elif curr_aspect_ratio < target_aspect_ratio:\n",
    "        new_height = int(width / target_aspect_ratio)\n",
    "        offset = (height - new_height) // 2\n",
    "        cropped_image = image[offset:offset + new_height, :,:]\n",
    "    else:\n",
    "        cropped_image = image\n",
    "\n",
    "    # Resize the cropped image\n",
    "    resized_image = resize(cropped_image, (width, height))\n",
    "\n",
    "   # print(\"Original shape:\", image.shape)\n",
    "   # print(\"Cropped shape:\", cropped_image.shape)\n",
    "   # print(\"Resized shape:\", resized_image.shape)\n",
    "    return resized_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c511df",
   "metadata": {},
   "source": [
    "### As part of our first experiment we are going to run Ablation experiment with batch size 100 and 3 epochs. Here are the details:\n",
    "#### Experiment 1(Ablation)\n",
    "1. Batch_Size = 100\n",
    "2. num_epochs = 3\n",
    "3. img_width, img_height = 100, 100\n",
    "4. Resize, Crop logic - Crop on corners - 10 pixels\n",
    "5. Train on 15 frames, alternate frames chosen\n",
    "6. Model Details: 1 Conv2D(Time Distributed, 16 filters, 3,3(kernel)), Flatten, 1 GRU(16 filter), Padding = Same, Optimizer = adam, categorical_accuracy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d63473ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 3\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = '/datasets/Project_data/train'\n",
    "val_path = '/datasets/Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 3 # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)\n",
    "img_width = 100\n",
    "img_height = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ab550f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-31 10:48:36.026996: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2024-08-31 10:48:36.027057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14800 MB memory:  -> device: 0, name: Quadro RTX 5000, pci bus id: 0000:41:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed (TimeDistr  (None, 15, 100, 100, 16)  448      \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, 15, 160000)       0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 15, 16)            7680864   \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 240)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 1205      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,682,517\n",
      "Trainable params: 7,682,517\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_model = Sequential()\n",
    "rnn_model.add(TimeDistributed(Conv2D(16, (3, 3), padding='same', activation='relu'), input_shape=(num_frames,img_height, img_width, 3)))\n",
    "rnn_model.add(TimeDistributed(Flatten()))  # Flatten the output before passing to GRU\n",
    "rnn_model.add(GRU(16, return_sequences=True))\n",
    "# rnn_model.add(TimeDistributed(Dense(64, activation='relu')))\n",
    "rnn_model.add((Flatten()))\n",
    "rnn_model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "rnn_model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ce34ed7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeModel():\n",
    "    model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "\n",
    "    if not os.path.exists(model_name):\n",
    "        os.mkdir(model_name)\n",
    "\n",
    "    filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', save_freq='epoch')\n",
    "\n",
    "    LR = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, mode='auto', min_lr=0.01)\n",
    "    callbacks_list = [checkpoint, LR]\n",
    "    return callbacks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3149582d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_epoch_steps_val_steps():\n",
    "    if (num_train_sequences%batch_size) == 0:\n",
    "        steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "    else:\n",
    "        steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "    if (num_val_sequences%batch_size) == 0:\n",
    "        validation_steps = int(num_val_sequences/batch_size)\n",
    "    else:\n",
    "        validation_steps = (num_val_sequences//batch_size) + 1\n",
    "    return steps_per_epoch, validation_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26a3a4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size, img_width, img_height, (10,10))\n",
    "val_generator = generator(val_path, val_doc, batch_size, img_width, img_height, (10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75bb1fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = initializeModel()\n",
    "steps_per_epoch, validation_steps = set_epoch_steps_val_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e1288e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /datasets/Project_data/train ; batch size = 100\n",
      "(100, 15, 100, 100, 3)\n",
      "(100, 5)\n",
      "Epoch 1/3\n",
      "(100, 15, 100, 100, 3)\n",
      "(100, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-31 10:48:53.315954: I tensorflow/stream_executor/cuda/cuda_dnn.cc:377] Loaded cuDNN version 8302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/7 [===>..........................] - ETA: 29s - loss: 1.7257 - categorical_accuracy: 0.1700(100, 15, 100, 100, 3)\n",
      "(100, 5)\n",
      "2/7 [=======>......................] - ETA: 33s - loss: 1.7142 - categorical_accuracy: 0.1950(100, 15, 100, 100, 3)\n",
      "(100, 5)\n",
      "3/7 [===========>..................] - ETA: 30s - loss: 1.6641 - categorical_accuracy: 0.2267(100, 15, 100, 100, 3)\n",
      "(100, 5)\n",
      "4/7 [================>.............] - ETA: 25s - loss: 1.6294 - categorical_accuracy: 0.2500(100, 15, 100, 100, 3)\n",
      "(100, 5)\n",
      "5/7 [====================>.........] - ETA: 17s - loss: 1.6202 - categorical_accuracy: 0.252063\n",
      "6/7 [========================>.....] - ETA: 8s - loss: 1.6178 - categorical_accuracy: 0.2450 (100, 15, 100, 100, 3)\n",
      "(100, 5)\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.6128 - categorical_accuracy: 0.2413Source path =  /datasets/Project_data/val ; batch size = 100\n",
      "(100, 15, 100, 100, 3)\n",
      "(100, 5)\n",
      "0\n",
      "(100, 15, 100, 100, 3)\n",
      "(100, 5)\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.56691, saving model to model_init_2024-08-3110_48_34.353482/model-00001-1.61282-0.24133-1.56691-0.31000.keras\n",
      "7/7 [==============================] - 81s 13s/step - loss: 1.6128 - categorical_accuracy: 0.2413 - val_loss: 1.5669 - val_categorical_accuracy: 0.3100 - lr: 0.0010\n",
      "Epoch 2/3\n",
      "(100, 15, 100, 100, 3)\n",
      "(100, 5)\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 1.4971 - categorical_accuracy: 0.3400(100, 15, 100, 100, 3)\n",
      "(100, 5)\n",
      "2/7 [=======>......................] - ETA: 48s - loss: 1.4976 - categorical_accuracy: 0.3350(100, 15, 100, 100, 3)\n",
      "(100, 5)\n",
      "3/7 [===========>..................] - ETA: 41s - loss: 1.5037 - categorical_accuracy: 0.3400(100, 15, 100, 100, 3)\n",
      "(100, 5)\n",
      "4/7 [================>.............] - ETA: 29s - loss: 1.5178 - categorical_accuracy: 0.3200(100, 15, 100, 100, 3)\n",
      "(100, 5)\n",
      "5/7 [====================>.........] - ETA: 19s - loss: 1.5226 - categorical_accuracy: 0.300063\n",
      "6/7 [========================>.....] - ETA: 9s - loss: 1.5206 - categorical_accuracy: 0.2983 (100, 15, 100, 100, 3)\n",
      "(100, 5)\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.5156 - categorical_accuracy: 0.29560\n",
      "(100, 15, 100, 100, 3)\n",
      "(100, 5)\n",
      "0\n",
      "(100, 15, 100, 100, 3)\n",
      "(100, 5)\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.56691 to 1.56332, saving model to model_init_2024-08-3110_48_34.353482/model-00002-1.51562-0.29563-1.56332-0.27000.keras\n",
      "7/7 [==============================] - 79s 13s/step - loss: 1.5156 - categorical_accuracy: 0.2956 - val_loss: 1.5633 - val_categorical_accuracy: 0.2700 - lr: 0.0010\n",
      "Epoch 3/3\n",
      "(100, 15, 100, 100, 3)\n",
      "(100, 5)\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 1.4936 - categorical_accuracy: 0.3800(100, 15, 100, 100, 3)\n",
      "(100, 5)\n",
      "2/7 [=======>......................] - ETA: 48s - loss: 1.4676 - categorical_accuracy: 0.3800(100, 15, 100, 100, 3)\n",
      "(100, 5)\n",
      "3/7 [===========>..................] - ETA: 38s - loss: 1.4847 - categorical_accuracy: 0.3500(100, 15, 100, 100, 3)\n",
      "(100, 5)\n",
      "4/7 [================>.............] - ETA: 28s - loss: 1.4890 - categorical_accuracy: 0.3525(100, 15, 100, 100, 3)\n",
      "(100, 5)\n",
      "5/7 [====================>.........] - ETA: 19s - loss: 1.4934 - categorical_accuracy: 0.338063\n",
      "6/7 [========================>.....] - ETA: 9s - loss: 1.4933 - categorical_accuracy: 0.3300 (100, 15, 100, 100, 3)\n",
      "(100, 5)\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4937 - categorical_accuracy: 0.32730\n",
      "(100, 15, 100, 100, 3)\n",
      "(100, 5)\n",
      "0\n",
      "(100, 15, 100, 100, 3)\n",
      "(100, 5)\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.56332 to 1.56126, saving model to model_init_2024-08-3110_48_34.353482/model-00003-1.49367-0.32730-1.56126-0.30000.keras\n",
      "7/7 [==============================] - 78s 13s/step - loss: 1.4937 - categorical_accuracy: 0.3273 - val_loss: 1.5613 - val_categorical_accuracy: 0.3000 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9f34b973d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rnn_model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
    "                    callbacks=callbacks_list, validation_data=val_generator,\n",
    "                    validation_steps=validation_steps, class_weight=None, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0574870",
   "metadata": {},
   "source": [
    "##### We see that we were able to acheive val categorical accuracy of 30% on validation data and val_loss of 1.49 in the ablation experiment. The model did learn, we are in the right direction. Let's try now with more epochs and see if we can improve on our previous outcome\n",
    "We also see that val_categorical_accuracy and categorical_accuracy are very close, that means the model is not overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "146dacce",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf /home/.local/share/Trash/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3f9f57",
   "metadata": {},
   "source": [
    "Experiment 2:\n",
    "1. batch_size = 32\n",
    "2. num_epochs = 5\n",
    "3. img_width, img_height = 100, 100\n",
    "4. Resize, Crop logic - Crop on corners - 10 pixels\n",
    "5. Train on 15 frames, alternate frames chosen\n",
    "6. Model Details: 1 Conv2D(Time Distributed, 16 filters, 3,3(kernel)), 1 Conv2D(Time Distributed, 16 filters, 3,3(kernel)), Flatten, 1 GRU(16 filter), Padding = Same, Optimizer = adam, categorical_accuracy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "894c6242",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_epochs = 5\n",
    "img_width = 100\n",
    "img_height = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4dbb0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_2 (TimeDis  (None, 15, 100, 100, 16)  448      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_3 (TimeDis  (None, 15, 100, 100, 32)  4640     \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_4 (TimeDis  (None, 15, 320000)       0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 15, 32)            30723264  \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 480)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 2405      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,730,757\n",
      "Trainable params: 30,730,757\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_model2 = Sequential()\n",
    "rnn_model2.add(TimeDistributed(Conv2D(16, (3, 3), padding='same', activation='relu'), input_shape=(num_frames,img_height, img_width, 3)))\n",
    "rnn_model2.add(TimeDistributed(Conv2D(32, (3, 3), padding='same', activation='relu'), input_shape=(num_frames,img_height, img_width, 3)))\n",
    "rnn_model2.add(TimeDistributed(Flatten()))  # Flatten the output before passing to GRU\n",
    "rnn_model2.add(GRU(32, return_sequences=True))\n",
    "# rnn_model.add(TimeDistributed(Dense(64, activation='relu')))\n",
    "rnn_model2.add((Flatten()))\n",
    "rnn_model2.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "rnn_model2.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "rnn_model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90d4729b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size, img_width, img_height, (10,10))\n",
    "val_generator = generator(val_path, val_doc, batch_size, img_width, img_height,(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c80224b",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = initializeModel()\n",
    "steps_per_epoch, validation_steps = set_epoch_steps_val_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c82d1810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /datasets/Project_data/train ; batch size = 32\n",
      "(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "Epoch 1/5\n",
      "(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      " 1/21 [>.............................] - ETA: 42s - loss: 1.7071 - categorical_accuracy: 0.1250(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      " 2/21 [=>............................] - ETA: 1:10 - loss: 1.8492 - categorical_accuracy: 0.1562(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      " 3/21 [===>..........................] - ETA: 1:01 - loss: 1.8541 - categorical_accuracy: 0.1562(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      " 4/21 [====>.........................] - ETA: 54s - loss: 1.7992 - categorical_accuracy: 0.2031 (32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      " 5/21 [======>.......................] - ETA: 49s - loss: 1.8050 - categorical_accuracy: 0.1875(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      " 6/21 [=======>......................] - ETA: 46s - loss: 1.8003 - categorical_accuracy: 0.1719(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      " 7/21 [=========>....................] - ETA: 43s - loss: 1.7773 - categorical_accuracy: 0.1741(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      " 8/21 [==========>...................] - ETA: 40s - loss: 1.7650 - categorical_accuracy: 0.1719(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      " 9/21 [===========>..................] - ETA: 36s - loss: 1.7589 - categorical_accuracy: 0.1667(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "10/21 [=============>................] - ETA: 33s - loss: 1.7528 - categorical_accuracy: 0.1656(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "11/21 [==============>...............] - ETA: 30s - loss: 1.7404 - categorical_accuracy: 0.1733(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "12/21 [================>.............] - ETA: 27s - loss: 1.7374 - categorical_accuracy: 0.1589(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "13/21 [=================>............] - ETA: 24s - loss: 1.7281 - categorical_accuracy: 0.1611(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "14/21 [===================>..........] - ETA: 21s - loss: 1.7182 - categorical_accuracy: 0.1652(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "15/21 [====================>.........] - ETA: 18s - loss: 1.7095 - categorical_accuracy: 0.1750(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "16/21 [=====================>........] - ETA: 15s - loss: 1.7060 - categorical_accuracy: 0.1719(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "17/21 [=======================>......] - ETA: 12s - loss: 1.7050 - categorical_accuracy: 0.1710(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "18/21 [========================>.....] - ETA: 9s - loss: 1.7020 - categorical_accuracy: 0.1684 (32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 1.6970 - categorical_accuracy: 0.174323\n",
      "20/21 [===========================>..] - ETA: 3s - loss: 1.6945 - categorical_accuracy: 0.1734(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.6937 - categorical_accuracy: 0.1704Source path =  /datasets/Project_data/val ; batch size = 32\n",
      "(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "4\n",
      "(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.61919, saving model to model_init_2024-08-3110_48_34.353482/model-00001-1.69369-0.17044-1.61919-0.21000.keras\n",
      "21/21 [==============================] - 80s 4s/step - loss: 1.6937 - categorical_accuracy: 0.1704 - val_loss: 1.6192 - val_categorical_accuracy: 0.2100 - lr: 0.0010\n",
      "Epoch 2/5\n",
      "(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      " 1/21 [>.............................] - ETA: 2s - loss: 1.6257 - categorical_accuracy: 0.0938(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      " 2/21 [=>............................] - ETA: 53s - loss: 1.6390 - categorical_accuracy: 0.1250(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      " 3/21 [===>..........................] - ETA: 50s - loss: 1.6431 - categorical_accuracy: 0.1042(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      " 4/21 [====>.........................] - ETA: 49s - loss: 1.6335 - categorical_accuracy: 0.1172(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      " 5/21 [======>.......................] - ETA: 45s - loss: 1.6318 - categorical_accuracy: 0.1250(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      " 6/21 [=======>......................] - ETA: 43s - loss: 1.6282 - categorical_accuracy: 0.1302(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      " 7/21 [=========>....................] - ETA: 40s - loss: 1.6259 - categorical_accuracy: 0.1339(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      " 8/21 [==========>...................] - ETA: 37s - loss: 1.6232 - categorical_accuracy: 0.1289(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      " 9/21 [===========>..................] - ETA: 34s - loss: 1.6180 - categorical_accuracy: 0.1493(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "10/21 [=============>................] - ETA: 31s - loss: 1.6178 - categorical_accuracy: 0.1469(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "11/21 [==============>...............] - ETA: 28s - loss: 1.6198 - categorical_accuracy: 0.1420(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "12/21 [================>.............] - ETA: 26s - loss: 1.6210 - categorical_accuracy: 0.1458(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "13/21 [=================>............] - ETA: 23s - loss: 1.6209 - categorical_accuracy: 0.1514(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "14/21 [===================>..........] - ETA: 20s - loss: 1.6237 - categorical_accuracy: 0.1473(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "15/21 [====================>.........] - ETA: 17s - loss: 1.6165 - categorical_accuracy: 0.1583(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "16/21 [=====================>........] - ETA: 15s - loss: 1.6117 - categorical_accuracy: 0.1680(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "17/21 [=======================>......] - ETA: 12s - loss: 1.6115 - categorical_accuracy: 0.1691(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "18/21 [========================>.....] - ETA: 9s - loss: 1.6107 - categorical_accuracy: 0.1771 (32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 1.6124 - categorical_accuracy: 0.174323\n",
      "20/21 [===========================>..] - ETA: 3s - loss: 1.6136 - categorical_accuracy: 0.1766(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.6148 - categorical_accuracy: 0.1780(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "4\n",
      "(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.61919 to 1.59471, saving model to model_init_2024-08-3110_48_34.353482/model-00002-1.61479-0.17798-1.59471-0.31000.keras\n",
      "21/21 [==============================] - 76s 4s/step - loss: 1.6148 - categorical_accuracy: 0.1780 - val_loss: 1.5947 - val_categorical_accuracy: 0.3100 - lr: 0.0010\n",
      "Epoch 3/5\n",
      "(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      " 1/21 [>.............................] - ETA: 2s - loss: 1.5768 - categorical_accuracy: 0.2500(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      " 2/21 [=>............................] - ETA: 50s - loss: 1.5921 - categorical_accuracy: 0.2656(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      " 3/21 [===>..........................] - ETA: 49s - loss: 1.6027 - categorical_accuracy: 0.2188(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      " 4/21 [====>.........................] - ETA: 51s - loss: 1.6025 - categorical_accuracy: 0.2578(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      " 5/21 [======>.......................] - ETA: 48s - loss: 1.6053 - categorical_accuracy: 0.2313(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      " 6/21 [=======>......................] - ETA: 46s - loss: 1.5977 - categorical_accuracy: 0.2292(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      " 7/21 [=========>....................] - ETA: 42s - loss: 1.5904 - categorical_accuracy: 0.2321(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      " 8/21 [==========>...................] - ETA: 40s - loss: 1.5936 - categorical_accuracy: 0.2188(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      " 9/21 [===========>..................] - ETA: 36s - loss: 1.5948 - categorical_accuracy: 0.2153(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "10/21 [=============>................] - ETA: 34s - loss: 1.5838 - categorical_accuracy: 0.2313(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "11/21 [==============>...............] - ETA: 31s - loss: 1.5848 - categorical_accuracy: 0.2358(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "12/21 [================>.............] - ETA: 28s - loss: 1.5941 - categorical_accuracy: 0.2292(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "13/21 [=================>............] - ETA: 25s - loss: 1.5913 - categorical_accuracy: 0.2332(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "14/21 [===================>..........] - ETA: 21s - loss: 1.5927 - categorical_accuracy: 0.2277(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "15/21 [====================>.........] - ETA: 18s - loss: 1.5932 - categorical_accuracy: 0.2313(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "16/21 [=====================>........] - ETA: 15s - loss: 1.5923 - categorical_accuracy: 0.2324(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "17/21 [=======================>......] - ETA: 12s - loss: 1.5942 - categorical_accuracy: 0.2316(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "18/21 [========================>.....] - ETA: 9s - loss: 1.5969 - categorical_accuracy: 0.2205 (32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 1.5957 - categorical_accuracy: 0.220423\n",
      "20/21 [===========================>..] - ETA: 3s - loss: 1.5934 - categorical_accuracy: 0.2203(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.5911 - categorical_accuracy: 0.2217(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "4\n",
      "(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.59471\n",
      "21/21 [==============================] - 77s 4s/step - loss: 1.5911 - categorical_accuracy: 0.2217 - val_loss: 1.6255 - val_categorical_accuracy: 0.1600 - lr: 0.0010\n",
      "Epoch 4/5\n",
      "(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      " 1/21 [>.............................] - ETA: 2s - loss: 1.5517 - categorical_accuracy: 0.1562(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      " 2/21 [=>............................] - ETA: 1:01 - loss: 1.5820 - categorical_accuracy: 0.1875(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      " 3/21 [===>..........................] - ETA: 52s - loss: 1.5771 - categorical_accuracy: 0.1875 (32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      " 4/21 [====>.........................] - ETA: 51s - loss: 1.5611 - categorical_accuracy: 0.2109(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      " 5/21 [======>.......................] - ETA: 47s - loss: 1.5560 - categorical_accuracy: 0.2188(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      " 6/21 [=======>......................] - ETA: 46s - loss: 1.5695 - categorical_accuracy: 0.2083(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      " 7/21 [=========>....................] - ETA: 43s - loss: 1.5740 - categorical_accuracy: 0.1964(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      " 8/21 [==========>...................] - ETA: 41s - loss: 1.5820 - categorical_accuracy: 0.1875(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      " 9/21 [===========>..................] - ETA: 38s - loss: 1.5726 - categorical_accuracy: 0.1979(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "10/21 [=============>................] - ETA: 34s - loss: 1.5668 - categorical_accuracy: 0.2062(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "11/21 [==============>...............] - ETA: 31s - loss: 1.5701 - categorical_accuracy: 0.2017(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "12/21 [================>.............] - ETA: 28s - loss: 1.5772 - categorical_accuracy: 0.2057(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "13/21 [=================>............] - ETA: 25s - loss: 1.5838 - categorical_accuracy: 0.2067(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "14/21 [===================>..........] - ETA: 22s - loss: 1.5886 - categorical_accuracy: 0.2121(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "15/21 [====================>.........] - ETA: 19s - loss: 1.5860 - categorical_accuracy: 0.2146(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "16/21 [=====================>........] - ETA: 15s - loss: 1.5838 - categorical_accuracy: 0.2207(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "17/21 [=======================>......] - ETA: 12s - loss: 1.5875 - categorical_accuracy: 0.2151(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "18/21 [========================>.....] - ETA: 9s - loss: 1.5871 - categorical_accuracy: 0.2170 (32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 1.5913 - categorical_accuracy: 0.217123\n",
      "20/21 [===========================>..] - ETA: 3s - loss: 1.5917 - categorical_accuracy: 0.2125(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.5910 - categorical_accuracy: 0.20974\n",
      "(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "4\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.59471\n",
      "21/21 [==============================] - 76s 4s/step - loss: 1.5910 - categorical_accuracy: 0.2097 - val_loss: 1.6156 - val_categorical_accuracy: 0.2800 - lr: 0.0010\n",
      "Epoch 5/5\n",
      "(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      " 1/21 [>.............................] - ETA: 2s - loss: 1.5099 - categorical_accuracy: 0.3125(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      " 2/21 [=>............................] - ETA: 1:02 - loss: 1.5779 - categorical_accuracy: 0.2656(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      " 3/21 [===>..........................] - ETA: 1:00 - loss: 1.5501 - categorical_accuracy: 0.3125(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      " 4/21 [====>.........................] - ETA: 55s - loss: 1.5425 - categorical_accuracy: 0.2812 (32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      " 5/21 [======>.......................] - ETA: 50s - loss: 1.5587 - categorical_accuracy: 0.2750(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      " 6/21 [=======>......................] - ETA: 47s - loss: 1.5646 - categorical_accuracy: 0.2604(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      " 7/21 [=========>....................] - ETA: 44s - loss: 1.5611 - categorical_accuracy: 0.2679(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      " 8/21 [==========>...................] - ETA: 41s - loss: 1.5628 - categorical_accuracy: 0.2617(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      " 9/21 [===========>..................] - ETA: 37s - loss: 1.5636 - categorical_accuracy: 0.2604(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "10/21 [=============>................] - ETA: 34s - loss: 1.5640 - categorical_accuracy: 0.2594(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "11/21 [==============>...............] - ETA: 31s - loss: 1.5700 - categorical_accuracy: 0.2528(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "12/21 [================>.............] - ETA: 28s - loss: 1.5745 - categorical_accuracy: 0.2474(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "13/21 [=================>............] - ETA: 25s - loss: 1.5754 - categorical_accuracy: 0.2452(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "14/21 [===================>..........] - ETA: 22s - loss: 1.5795 - categorical_accuracy: 0.2433(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "15/21 [====================>.........] - ETA: 18s - loss: 1.5823 - categorical_accuracy: 0.2375(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "16/21 [=====================>........] - ETA: 15s - loss: 1.5820 - categorical_accuracy: 0.2402(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "17/21 [=======================>......] - ETA: 12s - loss: 1.5838 - categorical_accuracy: 0.2408(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "18/21 [========================>.....] - ETA: 9s - loss: 1.5827 - categorical_accuracy: 0.2431 (32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 1.5864 - categorical_accuracy: 0.235223\n",
      "20/21 [===========================>..] - ETA: 3s - loss: 1.5868 - categorical_accuracy: 0.2391(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.5882 - categorical_accuracy: 0.2398(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "4\n",
      "(32, 15, 100, 100, 3)\n",
      "(32, 5)\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.59471\n",
      "21/21 [==============================] - 78s 4s/step - loss: 1.5882 - categorical_accuracy: 0.2398 - val_loss: 1.6308 - val_categorical_accuracy: 0.2400 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9f4ece1e80>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model2.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
    "                    callbacks=callbacks_list, validation_data=val_generator,\n",
    "                    validation_steps=validation_steps, class_weight=None, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c343c5",
   "metadata": {},
   "source": [
    "It is interesting to note that reducing the batch_size and increasing the number of epochs + adding of a conv2D layer did not help. Our categorical accuracy went down and loss also did not improve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bcc122",
   "metadata": {},
   "source": [
    "Experiment 3:\n",
    "Let's try changing out crop logic. As we can see after observing random images, top of the image does not contain any info. So we will crop 20 pixels from top only. Also we will increase our batch_size to 32, num_epochs = 5. We will further train at 10 epochs if the results are better than before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a1b02c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf /home/.local/share/Trash/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "447ea334",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_epochs = 5\n",
    "img_width = 100\n",
    "img_height = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f5b81838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_resize(image, width, height,crop_dims):\n",
    "    curr_width, curr_height, channel = image.shape\n",
    "    \n",
    "    crop_x = crop_dims[0]\n",
    "    crop_y = crop_dims[1]\n",
    "        \n",
    "    cropped_image = image[0:curr_height-crop_y,0:curr_width-crop_x]\n",
    "\n",
    "    # Resize the cropped image\n",
    "    resized_image = resize(cropped_image, (width, height))\n",
    "\n",
    "    #print(\"Original shape:\", image.shape)\n",
    "    #print(\"Cropped shape:\", cropped_image.shape)\n",
    "    #print(\"Resized shape:\", resized_image.shape)\n",
    "    return resized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0e7d82b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_17 (TimeDi  (None, 22, 100, 100, 16)  448      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_18 (TimeDi  (None, 22, 100, 100, 32)  4640     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_19 (TimeDi  (None, 22, 320000)       0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " gru_6 (GRU)                 (None, 22, 32)            30723264  \n",
      "                                                                 \n",
      " flatten_13 (Flatten)        (None, 704)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 5)                 3525      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,731,877\n",
      "Trainable params: 30,731,877\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_model2 = Sequential()\n",
    "rnn_model2.add(TimeDistributed(Conv2D(16, (3, 3), padding='same', activation='relu'), input_shape=(num_frames,img_height, img_width, 3)))\n",
    "rnn_model2.add(TimeDistributed(Conv2D(32, (3, 3), padding='same', activation='relu'), input_shape=(num_frames,img_height, img_width, 3)))\n",
    "rnn_model2.add(TimeDistributed(Flatten()))  # Flatten the output before passing to GRU\n",
    "rnn_model2.add(GRU(32, return_sequences=True))\n",
    "# rnn_model.add(TimeDistributed(Dense(64, activation='relu')))\n",
    "rnn_model2.add((Flatten()))\n",
    "rnn_model2.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "rnn_model2.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "rnn_model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8a3377dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size, img_width, img_height, (40,10))\n",
    "val_generator = generator(val_path, val_doc, batch_size, img_width, img_height,(40,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2d690bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = initializeModel()\n",
    "steps_per_epoch, validation_steps = set_epoch_steps_val_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "28171474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /datasets/Project_data/train ; batch size = 32\n",
      "Epoch 1/5\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 1.6736 - categorical_accuracy: 0.248423\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.6682 - categorical_accuracy: 0.2443Source path =  /datasets/Project_data/val ; batch size = 32\n",
      "4\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.64746, saving model to model_init_2024-08-3110_48_34.353482/model-00001-1.66816-0.24434-1.64746-0.19000.keras\n",
      "21/21 [==============================] - 81s 4s/step - loss: 1.6682 - categorical_accuracy: 0.2443 - val_loss: 1.6475 - val_categorical_accuracy: 0.1900 - lr: 0.0010\n",
      "Epoch 2/5\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 1.6197 - categorical_accuracy: 0.213823\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.6184 - categorical_accuracy: 0.20814\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.64746 to 1.57242, saving model to model_init_2024-08-3110_48_34.353482/model-00002-1.61837-0.20814-1.57242-0.29000.keras\n",
      "21/21 [==============================] - 81s 4s/step - loss: 1.6184 - categorical_accuracy: 0.2081 - val_loss: 1.5724 - val_categorical_accuracy: 0.2900 - lr: 0.0010\n",
      "Epoch 3/5\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 1.6177 - categorical_accuracy: 0.210523\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.6173 - categorical_accuracy: 0.21274\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.57242\n",
      "21/21 [==============================] - 78s 4s/step - loss: 1.6173 - categorical_accuracy: 0.2127 - val_loss: 1.6196 - val_categorical_accuracy: 0.1400 - lr: 0.0010\n",
      "Epoch 4/5\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 1.6082 - categorical_accuracy: 0.230323\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.6100 - categorical_accuracy: 0.22784\n",
      "4\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.57242\n",
      "21/21 [==============================] - 77s 4s/step - loss: 1.6100 - categorical_accuracy: 0.2278 - val_loss: 1.5885 - val_categorical_accuracy: 0.2500 - lr: 0.0010\n",
      "Epoch 5/5\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 1.6002 - categorical_accuracy: 0.228623\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.6000 - categorical_accuracy: 0.22784\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.57242\n",
      "21/21 [==============================] - 78s 4s/step - loss: 1.6000 - categorical_accuracy: 0.2278 - val_loss: 1.5985 - val_categorical_accuracy: 0.2600 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9f348bb6a0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model2.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
    "                    callbacks=callbacks_list, validation_data=val_generator,\n",
    "                    validation_steps=validation_steps, class_weight=None, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b884392",
   "metadata": {},
   "source": [
    "Looks like our resizing did not affect the model positively. We will go back to previous resizing logic but keep cropping from top only. Also we will change the frames we will train on. We will get rid of beginning and end 4 frames. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447b7384",
   "metadata": {},
   "source": [
    "Experiment 4:\n",
    "batch_size = 32\n",
    "num_epochs = 5\n",
    "img_width = 100\n",
    "img_height = 100\n",
    "crop logic = top 40 pixels, crop on basis of aspect ratio\n",
    "img_idx = drop first 4 and last 4 frames, total 22 frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f5018cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf /home/.local/share/Trash/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e7249967",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_epochs = 5\n",
    "img_width = 100\n",
    "img_height = 100\n",
    "img_idx = [4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25]\n",
    "num_frames = len(img_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5acb139f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size, img_width, img_height, (40,10))\n",
    "val_generator = generator(val_path, val_doc, batch_size, img_width, img_height,(40,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e76c2c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = initializeModel()\n",
    "steps_per_epoch, validation_steps = set_epoch_steps_val_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c0a4fa6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_20 (TimeDi  (None, 22, 100, 100, 16)  448      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_21 (TimeDi  (None, 22, 100, 100, 32)  4640     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_22 (TimeDi  (None, 22, 320000)       0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " gru_7 (GRU)                 (None, 22, 32)            30723264  \n",
      "                                                                 \n",
      " flatten_15 (Flatten)        (None, 704)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 5)                 3525      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,731,877\n",
      "Trainable params: 30,731,877\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_model3 = Sequential()\n",
    "rnn_model3.add(TimeDistributed(Conv2D(16, (3, 3), padding='same', activation='relu'), input_shape=(num_frames,img_height, img_width, 3)))\n",
    "rnn_model3.add(TimeDistributed(Conv2D(32, (3, 3), padding='same', activation='relu'), input_shape=(num_frames,img_height, img_width, 3)))\n",
    "rnn_model3.add(TimeDistributed(Flatten()))  # Flatten the output before passing to GRU\n",
    "rnn_model3.add(GRU(32, return_sequences=True))\n",
    "# rnn_model.add(TimeDistributed(Dense(64, activation='relu')))\n",
    "rnn_model3.add((Flatten()))\n",
    "rnn_model3.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "rnn_model3.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "rnn_model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "dfa4093c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /datasets/Project_data/train ; batch size = 32\n",
      "Epoch 1/5\n",
      "19/21 [==========================>...] - ETA: 9s - loss: 1.8467 - categorical_accuracy: 0.2072 23\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.8349 - categorical_accuracy: 0.2006Source path =  /datasets/Project_data/val ; batch size = 32\n",
      "4\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.66071, saving model to model_init_2024-08-3110_48_34.353482/model-00001-1.83490-0.20060-1.66071-0.17000.keras\n",
      "21/21 [==============================] - 116s 6s/step - loss: 1.8349 - categorical_accuracy: 0.2006 - val_loss: 1.6607 - val_categorical_accuracy: 0.1700 - lr: 0.0010\n",
      "Epoch 2/5\n",
      "19/21 [==========================>...] - ETA: 9s - loss: 1.6389 - categorical_accuracy: 0.1711 23\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.6380 - categorical_accuracy: 0.16894\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.66071 to 1.60603, saving model to model_init_2024-08-3110_48_34.353482/model-00002-1.63799-0.16893-1.60603-0.24000.keras\n",
      "21/21 [==============================] - 112s 6s/step - loss: 1.6380 - categorical_accuracy: 0.1689 - val_loss: 1.6060 - val_categorical_accuracy: 0.2400 - lr: 0.0010\n",
      "Epoch 3/5\n",
      "19/21 [==========================>...] - ETA: 8s - loss: 1.6261 - categorical_accuracy: 0.1859 23\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.6255 - categorical_accuracy: 0.18704\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.60603\n",
      "21/21 [==============================] - 109s 5s/step - loss: 1.6255 - categorical_accuracy: 0.1870 - val_loss: 1.6288 - val_categorical_accuracy: 0.1800 - lr: 0.0010\n",
      "Epoch 4/5\n",
      "19/21 [==========================>...] - ETA: 9s - loss: 1.6226 - categorical_accuracy: 0.2023 23\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.6244 - categorical_accuracy: 0.20974\n",
      "4\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.60603\n",
      "21/21 [==============================] - 106s 5s/step - loss: 1.6244 - categorical_accuracy: 0.2097 - val_loss: 1.6285 - val_categorical_accuracy: 0.2200 - lr: 0.0010\n",
      "Epoch 5/5\n",
      "19/21 [==========================>...] - ETA: 9s - loss: 1.6327 - categorical_accuracy: 0.1546 23\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.6282 - categorical_accuracy: 0.15694\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.60603\n",
      "21/21 [==============================] - 116s 6s/step - loss: 1.6282 - categorical_accuracy: 0.1569 - val_loss: 1.6296 - val_categorical_accuracy: 0.1700 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9f91f29790>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model3.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
    "                    callbacks=callbacks_list, validation_data=val_generator,\n",
    "                    validation_steps=validation_steps, class_weight=None, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45084f9f",
   "metadata": {},
   "source": [
    "There was no improvement in the model, we will go back to using alternate frames in img_idx, keep the batch_size at 32 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181f5231",
   "metadata": {},
   "source": [
    "Experiment 4\n",
    "1. batch_size = 32\n",
    "2. img_idx = alternate frames\n",
    "3. img_width, img_height = 100, 100\n",
    "4. num_epochs = 5\n",
    "5. Add batch_normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4559fddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf /home/.local/share/Trash/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2884b5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('/datasets/Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('/datasets/Project_data/val.csv').readlines())\n",
    "batch_size = 32 #experiment with the batch size\n",
    "img_idx = [0,2,4,6,8,10,12,14,16,18,20,22,24,26,28]#create a list of image numbers you want to use\n",
    "num_frames = len(img_idx)\n",
    "num_classes = 5\n",
    "batch_size = 32\n",
    "num_epochs = 5\n",
    "img_width = 100\n",
    "img_height = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6f3f2f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_23 (TimeDi  (None, 15, 100, 100, 16)  448      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_24 (TimeDi  (None, 15, 100, 100, 16)  64       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_25 (TimeDi  (None, 15, 100, 100, 32)  4640     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_26 (TimeDi  (None, 15, 100, 100, 32)  128      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_27 (TimeDi  (None, 15, 320000)       0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " gru_8 (GRU)                 (None, 15, 64)            61452672  \n",
      "                                                                 \n",
      " flatten_17 (Flatten)        (None, 960)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 5)                 4805      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 61,462,757\n",
      "Trainable params: 61,462,661\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import TimeDistributed, Conv2D, Flatten, GRU, Dense, BatchNormalization\n",
    "\n",
    "rnn_model4 = Sequential()\n",
    "rnn_model4.add(TimeDistributed(Conv2D(16, (3, 3), padding='same', activation='relu'), input_shape=(num_frames, img_height, img_width, 3)))\n",
    "rnn_model4.add(TimeDistributed(BatchNormalization()))\n",
    "rnn_model4.add(TimeDistributed(Conv2D(32, (3, 3), padding='same', activation='relu')))\n",
    "rnn_model4.add(TimeDistributed(BatchNormalization()))\n",
    "rnn_model4.add(TimeDistributed(Flatten()))  # Flatten the output before passing to GRU\n",
    "rnn_model4.add(GRU(64, return_sequences=True))\n",
    "rnn_model4.add(Flatten())\n",
    "rnn_model4.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "rnn_model4.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "rnn_model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "980e6ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size, img_width, img_height, (40,10))\n",
    "val_generator = generator(val_path, val_doc, batch_size, img_width, img_height,(40,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d2c95659",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = initializeModel()\n",
    "steps_per_epoch, validation_steps = set_epoch_steps_val_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bd1f481c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /datasets/Project_data/train ; batch size = 32\n",
      "Epoch 1/5\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 1.8242 - categorical_accuracy: 0.264823\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.8095 - categorical_accuracy: 0.2564Source path =  /datasets/Project_data/val ; batch size = 32\n",
      "4\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.65546, saving model to model_init_2024-08-3110_48_34.353482/model-00001-1.80952-0.25641-1.65546-0.29000.keras\n",
      "21/21 [==============================] - 79s 4s/step - loss: 1.8095 - categorical_accuracy: 0.2564 - val_loss: 1.6555 - val_categorical_accuracy: 0.2900 - lr: 0.0010\n",
      "Epoch 2/5\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 1.4740 - categorical_accuracy: 0.355323\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.4697 - categorical_accuracy: 0.36054\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.65546 to 1.55797, saving model to model_init_2024-08-3110_48_34.353482/model-00002-1.46968-0.36048-1.55797-0.35000.keras\n",
      "21/21 [==============================] - 77s 4s/step - loss: 1.4697 - categorical_accuracy: 0.3605 - val_loss: 1.5580 - val_categorical_accuracy: 0.3500 - lr: 0.0010\n",
      "Epoch 3/5\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 1.4038 - categorical_accuracy: 0.383223\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.3896 - categorical_accuracy: 0.38614\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.55797\n",
      "21/21 [==============================] - 76s 4s/step - loss: 1.3896 - categorical_accuracy: 0.3861 - val_loss: 1.6152 - val_categorical_accuracy: 0.2900 - lr: 0.0010\n",
      "Epoch 4/5\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 1.3265 - categorical_accuracy: 0.447423\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.3291 - categorical_accuracy: 0.44494\n",
      "4\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.55797\n",
      "21/21 [==============================] - 72s 4s/step - loss: 1.3291 - categorical_accuracy: 0.4449 - val_loss: 1.5725 - val_categorical_accuracy: 0.2800 - lr: 0.0010\n",
      "Epoch 5/5\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 1.3058 - categorical_accuracy: 0.473723\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.3141 - categorical_accuracy: 0.47364\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.55797\n",
      "21/21 [==============================] - 77s 4s/step - loss: 1.3141 - categorical_accuracy: 0.4736 - val_loss: 1.6748 - val_categorical_accuracy: 0.3300 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9f9252f280>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model4.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
    "                    callbacks=callbacks_list, validation_data=val_generator,\n",
    "                    validation_steps=validation_steps, class_weight=None, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d98685",
   "metadata": {},
   "source": [
    "We can see that there in definite improvement in the categorical accuracy in just 5 epochs. However val_categorical_accuracy is lower. We will now try Conv3D layer instead of Conv2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5e4e60",
   "metadata": {},
   "source": [
    "Experiment 5\n",
    "\n",
    "batch_size = 32\n",
    "img_idx = alternate frames\n",
    "img_width, img_height = 100, 100\n",
    "num_epochs = 5\n",
    "Add batch_normalization\n",
    "Time distributed Conv 3D layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ab0257b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf /home/.local/share/Trash/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a785d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('/datasets/Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('/datasets/Project_data/val.csv').readlines())\n",
    "batch_size = 32 #experiment with the batch size\n",
    "img_idx = [0,2,4,6,8,10,12,14,16,18,20,22,24,26,28]#create a list of image numbers you want to use\n",
    "num_frames = len(img_idx)\n",
    "num_classes = 5\n",
    "batch_size = 32\n",
    "num_epochs = 5\n",
    "img_width = 100\n",
    "img_height = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9356e8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-01 05:42:38.229375: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2024-09-01 05:42:38.229449: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14800 MB memory:  -> device: 0, name: Quadro RTX 5000, pci bus id: 0000:1b:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d (Conv3D)             (None, 15, 100, 100, 16)  1312      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 15, 100, 100, 16)  64       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv3d_1 (Conv3D)           (None, 15, 100, 100, 32)  13856     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 15, 100, 100, 32)  128      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 15, 320000)       0         \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 64)                61452672  \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 61,468,357\n",
      "Trainable params: 61,468,261\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import TimeDistributed, Conv3D, Flatten, GRU, Dense, BatchNormalization\n",
    "\n",
    "# Conv3D: Define the input shape (time_steps, depth, height, width, channels)\n",
    "\n",
    "rnn_model5 = Sequential()\n",
    "rnn_model5.add(Conv3D(16, (3, 3, 3), padding='same', activation='relu', input_shape=(num_frames,img_height, img_width, 3)))\n",
    "rnn_model5.add(BatchNormalization())\n",
    "rnn_model5.add(Conv3D(32, (3, 3, 3), padding='same', activation='relu'))\n",
    "rnn_model5.add(BatchNormalization())\n",
    "rnn_model5.add(TimeDistributed(Flatten()))  # Flatten the output before passing to GRU\n",
    "rnn_model5.add(GRU(64, return_sequences=False))\n",
    "rnn_model5.add(Flatten())\n",
    "rnn_model5.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "rnn_model5.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "rnn_model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3db2b349",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size, img_width, img_height, (40,10))\n",
    "val_generator = generator(val_path, val_doc, batch_size, img_width, img_height,(40,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b93a0cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = initializeModel()\n",
    "steps_per_epoch, validation_steps = set_epoch_steps_val_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc3d340e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /datasets/Project_data/train ; batch size = 32\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-01 05:42:56.499371: I tensorflow/stream_executor/cuda/cuda_dnn.cc:377] Loaded cuDNN version 8302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/21 [==========================>...] - ETA: 9s - loss: 1.8521 - categorical_accuracy: 0.2500 23\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.8258 - categorical_accuracy: 0.2594Source path =  /datasets/Project_data/val ; batch size = 32\n",
      "4\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.62865, saving model to model_init_2024-09-0105_41_48.686207/model-00001-1.82578-0.25943-2.62865-0.27000.keras\n",
      "21/21 [==============================] - 122s 6s/step - loss: 1.8258 - categorical_accuracy: 0.2594 - val_loss: 2.6287 - val_categorical_accuracy: 0.2700 - lr: 0.0010\n",
      "Epoch 2/5\n",
      "19/21 [==========================>...] - ETA: 8s - loss: 1.4560 - categorical_accuracy: 0.3832 23\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.4534 - categorical_accuracy: 0.38014\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.62865 to 1.90378, saving model to model_init_2024-09-0105_41_48.686207/model-00002-1.45340-0.38009-1.90378-0.20000.keras\n",
      "21/21 [==============================] - 99s 5s/step - loss: 1.4534 - categorical_accuracy: 0.3801 - val_loss: 1.9038 - val_categorical_accuracy: 0.2000 - lr: 0.0010\n",
      "Epoch 3/5\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 1.4595 - categorical_accuracy: 0.350323\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.4618 - categorical_accuracy: 0.34544\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.90378 to 1.77002, saving model to model_init_2024-09-0105_41_48.686207/model-00003-1.46185-0.34540-1.77002-0.30000.keras\n",
      "21/21 [==============================] - 79s 4s/step - loss: 1.4618 - categorical_accuracy: 0.3454 - val_loss: 1.7700 - val_categorical_accuracy: 0.3000 - lr: 0.0010\n",
      "Epoch 4/5\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 1.4460 - categorical_accuracy: 0.371723\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.4575 - categorical_accuracy: 0.36804\n",
      "4\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.77002 to 1.68437, saving model to model_init_2024-09-0105_41_48.686207/model-00004-1.45754-0.36802-1.68437-0.29000.keras\n",
      "21/21 [==============================] - 75s 4s/step - loss: 1.4575 - categorical_accuracy: 0.3680 - val_loss: 1.6844 - val_categorical_accuracy: 0.2900 - lr: 0.0010\n",
      "Epoch 5/5\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 1.5019 - categorical_accuracy: 0.332223\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.4996 - categorical_accuracy: 0.32584\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.68437\n",
      "21/21 [==============================] - 75s 4s/step - loss: 1.4996 - categorical_accuracy: 0.3258 - val_loss: 1.7337 - val_categorical_accuracy: 0.3300 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f21d894f490>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model5.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
    "                    callbacks=callbacks_list, validation_data=val_generator,\n",
    "                    validation_steps=validation_steps, class_weight=None, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d7d200",
   "metadata": {},
   "source": [
    "We can see that the gap between the train and val accuracy has reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708176f8",
   "metadata": {},
   "source": [
    "Experiment 6:\n",
    "batch_size = 32\n",
    "num_epochs = 5\n",
    "img_width, img_height = 100\n",
    "frames = alternate, 15\n",
    "layers - 2 - Conv 3D layer - Time distributed flatten - 2-GRU layer - Dense layer\n",
    "optimizer = sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4a656842",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf /home/.local/share/Trash/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b126d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('/datasets/Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('/datasets/Project_data/val.csv').readlines())\n",
    "batch_size = 32 #experiment with the batch size\n",
    "img_idx = [0,2,4,6,8,10,12,14,16,18,20,22,24,26,28]#create a list of image numbers you want to use\n",
    "num_frames = len(img_idx)\n",
    "num_classes = 5\n",
    "batch_size = 32\n",
    "num_epochs = 5\n",
    "img_width = 100\n",
    "img_height = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e2f9916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_2 (Conv3D)           (None, 15, 100, 100, 16)  1312      \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 15, 100, 100, 16)  64       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv3d_3 (Conv3D)           (None, 15, 100, 100, 32)  13856     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 15, 100, 100, 32)  128      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, 15, 320000)       0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 15, 64)            61452672  \n",
      "                                                                 \n",
      " gru_2 (GRU)                 (None, 15, 128)           74496     \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 1920)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 9605      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 61,552,133\n",
      "Trainable params: 61,552,037\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import TimeDistributed, Conv3D, Flatten, GRU, Dense, BatchNormalization\n",
    "\n",
    "# Conv3D: Define the input shape (time_steps, depth, height, width, channels)\n",
    "\n",
    "rnn_model6 = Sequential()\n",
    "rnn_model6.add(Conv3D(16, (3, 3, 3), padding='same', activation='relu', input_shape=(num_frames,img_height, img_width, 3)))\n",
    "rnn_model6.add(BatchNormalization())\n",
    "rnn_model6.add(Conv3D(32, (3, 3, 3), padding='same', activation='relu'))\n",
    "rnn_model6.add(BatchNormalization())\n",
    "rnn_model6.add(TimeDistributed(Flatten()))  # Flatten the output before passing to GRU\n",
    "rnn_model6.add(GRU(64, return_sequences=True))\n",
    "rnn_model6.add(GRU(128, return_sequences=True))\n",
    "rnn_model6.add(Flatten())\n",
    "rnn_model6.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "rnn_model6.compile(optimizer=\"sgd\", loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "rnn_model6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ac98e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size, img_width, img_height, (40,10))\n",
    "val_generator = generator(val_path, val_doc, batch_size, img_width, img_height,(40,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "271867c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = initializeModel()\n",
    "steps_per_epoch, validation_steps = set_epoch_steps_val_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c15ac1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /datasets/Project_data/train ; batch size = 32\n",
      "Epoch 1/5\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 1.3778 - categorical_accuracy: 0.424323\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.3431 - categorical_accuracy: 0.4404Source path =  /datasets/Project_data/val ; batch size = 32\n",
      "4\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.73248, saving model to model_init_2024-09-0105_41_48.686207/model-00001-1.34315-0.44042-1.73248-0.31000.keras\n",
      "21/21 [==============================] - 79s 4s/step - loss: 1.3431 - categorical_accuracy: 0.4404 - val_loss: 1.7325 - val_categorical_accuracy: 0.3100 - lr: 0.0100\n",
      "Epoch 2/5\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 0.5550 - categorical_accuracy: 0.858623\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5388 - categorical_accuracy: 0.86124\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.73248 to 1.56720, saving model to model_init_2024-09-0105_41_48.686207/model-00002-0.53883-0.86124-1.56720-0.37000.keras\n",
      "21/21 [==============================] - 77s 4s/step - loss: 0.5388 - categorical_accuracy: 0.8612 - val_loss: 1.5672 - val_categorical_accuracy: 0.3700 - lr: 0.0100\n",
      "Epoch 3/5\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 0.1653 - categorical_accuracy: 0.986823\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1595 - categorical_accuracy: 0.98644\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.56720 to 1.18998, saving model to model_init_2024-09-0105_41_48.686207/model-00003-0.15947-0.98643-1.18998-0.50000.keras\n",
      "21/21 [==============================] - 76s 4s/step - loss: 0.1595 - categorical_accuracy: 0.9864 - val_loss: 1.1900 - val_categorical_accuracy: 0.5000 - lr: 0.0100\n",
      "Epoch 4/5\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 0.0560 - categorical_accuracy: 1.000023\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0548 - categorical_accuracy: 1.00004\n",
      "4\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.18998\n",
      "21/21 [==============================] - 73s 4s/step - loss: 0.0548 - categorical_accuracy: 1.0000 - val_loss: 1.2119 - val_categorical_accuracy: 0.4700 - lr: 0.0100\n",
      "Epoch 5/5\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 0.0293 - categorical_accuracy: 1.000023\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0290 - categorical_accuracy: 1.00004\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.18998 to 1.09934, saving model to model_init_2024-09-0105_41_48.686207/model-00005-0.02896-1.00000-1.09934-0.52000.keras\n",
      "21/21 [==============================] - 76s 4s/step - loss: 0.0290 - categorical_accuracy: 1.0000 - val_loss: 1.0993 - val_categorical_accuracy: 0.5200 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f217b096d30>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model6.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
    "                    callbacks=callbacks_list, validation_data=val_generator,\n",
    "                    validation_steps=validation_steps, class_weight=None, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17e5897",
   "metadata": {},
   "source": [
    "Our categorical accuracy took a huge jump by adding a GRU unit but there is a huge gap between val accuracy and train accuracy. Let's introduce maxpooling and dropout for better generalization and overfitting prevention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f98431",
   "metadata": {},
   "source": [
    "Experiment 7:\n",
    "1. batch_size = 32\n",
    "2. num_epochs = 5\n",
    "3. img_width, img_height = 100, 100\n",
    "4. Add batch_normalization\n",
    "5. Conv3d, Conv3d + Maxpooling + Dropout(0.10), TimeDistributed Flatten + GRU + GRU + Dropout(0.10) + Flatten + Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a8529396",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('/datasets/Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('/datasets/Project_data/val.csv').readlines())\n",
    "batch_size = 32 #experiment with the batch size\n",
    "img_idx = [0,2,4,6,8,10,12,14,16,18,20,22,24,26,28]#create a list of image numbers you want to use\n",
    "num_frames = len(img_idx)\n",
    "num_classes = 5\n",
    "batch_size = 32\n",
    "num_epochs = 5\n",
    "img_width = 100\n",
    "img_height = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "439abca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_8 (Conv3D)           (None, 15, 100, 100, 16)  1312      \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 15, 100, 100, 16)  64       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv3d_9 (Conv3D)           (None, 15, 100, 100, 32)  13856     \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 15, 100, 100, 32)  128      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_4 (MaxPooling  (None, 7, 50, 50, 32)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 7, 50, 50, 32)     0         \n",
      "                                                                 \n",
      " time_distributed_3 (TimeDis  (None, 7, 80000)         0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " gru_5 (GRU)                 (None, 7, 64)             15372672  \n",
      "                                                                 \n",
      " gru_6 (GRU)                 (None, 7, 128)            74496     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 7, 128)            0         \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 896)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 4485      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,467,013\n",
      "Trainable params: 15,466,917\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv3D, BatchNormalization, TimeDistributed, Flatten, GRU, Dense, Dropout, MaxPooling3D\n",
    "\n",
    "rnn_model7 = Sequential()\n",
    "rnn_model7.add(Conv3D(16, (3, 3, 3), padding='same', activation='relu', input_shape=(num_frames, img_height, img_width, 3)))\n",
    "rnn_model7.add(BatchNormalization())\n",
    "\n",
    "rnn_model7.add(Conv3D(32, (3, 3, 3), padding='same', activation='relu'))\n",
    "rnn_model7.add(BatchNormalization())\n",
    "rnn_model7.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "rnn_model7.add(Dropout(0.10))\n",
    "\n",
    "rnn_model7.add(TimeDistributed(Flatten()))  # Flatten the output before passing to GRU\n",
    "rnn_model7.add(GRU(64, return_sequences=True))\n",
    "rnn_model7.add(GRU(128, return_sequences=True))\n",
    "rnn_model7.add(Dropout(0.10))\n",
    "rnn_model7.add(Flatten())\n",
    "rnn_model7.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "rnn_model7.compile(optimizer=\"sgd\", loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "# Now print the model summary\n",
    "rnn_model7.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a95554c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size, img_width, img_height, (40,10))\n",
    "val_generator = generator(val_path, val_doc, batch_size, img_width, img_height,(40,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "44a1f124",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = initializeModel()\n",
    "steps_per_epoch, validation_steps = set_epoch_steps_val_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "006c5152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /datasets/Project_data/train ; batch size = 32\n",
      "Epoch 1/5\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 1.4889 - categorical_accuracy: 0.347023\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.4560 - categorical_accuracy: 0.3725Source path =  /datasets/Project_data/val ; batch size = 32\n",
      "4\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.47625, saving model to model_init_2024-09-0106_15_55.769024/model-00001-1.45604-0.37255-1.47625-0.30000.h5\n",
      "21/21 [==============================] - 81s 4s/step - loss: 1.4560 - categorical_accuracy: 0.3725 - val_loss: 1.4762 - val_categorical_accuracy: 0.3000 - lr: 0.0100\n",
      "Epoch 2/5\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 0.9703 - categorical_accuracy: 0.659523\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9459 - categorical_accuracy: 0.67274\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.47625 to 1.46415, saving model to model_init_2024-09-0106_15_55.769024/model-00002-0.94587-0.67270-1.46415-0.31000.h5\n",
      "21/21 [==============================] - 80s 4s/step - loss: 0.9459 - categorical_accuracy: 0.6727 - val_loss: 1.4642 - val_categorical_accuracy: 0.3100 - lr: 0.0100\n",
      "Epoch 3/5\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 0.6250 - categorical_accuracy: 0.815823\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6203 - categorical_accuracy: 0.81754\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.46415\n",
      "21/21 [==============================] - 79s 4s/step - loss: 0.6203 - categorical_accuracy: 0.8175 - val_loss: 1.8931 - val_categorical_accuracy: 0.2900 - lr: 0.0100\n",
      "Epoch 4/5\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 0.3587 - categorical_accuracy: 0.949023\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3515 - categorical_accuracy: 0.95174\n",
      "4\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.46415\n",
      "21/21 [==============================] - 75s 4s/step - loss: 0.3515 - categorical_accuracy: 0.9517 - val_loss: 1.7646 - val_categorical_accuracy: 0.3200 - lr: 0.0100\n",
      "Epoch 5/5\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 0.1829 - categorical_accuracy: 0.988523\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1784 - categorical_accuracy: 0.98944\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.46415\n",
      "21/21 [==============================] - 79s 4s/step - loss: 0.1784 - categorical_accuracy: 0.9894 - val_loss: 1.5923 - val_categorical_accuracy: 0.3600 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f21d890d250>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model7.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
    "                    callbacks=callbacks_list, validation_data=val_generator,\n",
    "                    validation_steps=validation_steps, class_weight=None, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afe9f0a",
   "metadata": {},
   "source": [
    "Our train accuracy is holding good at 98.9% but val accuracy is still very low. Let's add more dropout and max pooling layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f844ca36",
   "metadata": {},
   "source": [
    "Experiment 8:\n",
    "1. batch_size = 32\n",
    "2. num_epochs = 5\n",
    "3. img_width, img_height = 100, 100\n",
    "4. Add batch_normalization\n",
    "5. Conv3d + Maxpooling + Dropout(0.25), Conv3d + Maxpooling + Dropout(0.25), TimeDistributed Flatten + GRU + Dropout(0.25) + GRU + Dropout(0.25) + Flatten + Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d821aff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf /home/.local/share/Trash/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3af95675",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('/datasets/Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('/datasets/Project_data/val.csv').readlines())\n",
    "batch_size = 32 #experiment with the batch size\n",
    "img_idx = [0,2,4,6,8,10,12,14,16,18,20,22,24,26,28]#create a list of image numbers you want to use\n",
    "num_frames = len(img_idx)\n",
    "num_classes = 5\n",
    "batch_size = 32\n",
    "num_epochs = 5\n",
    "img_width = 100\n",
    "img_height = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b67501a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_12 (Conv3D)          (None, 15, 100, 100, 16)  1312      \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 15, 100, 100, 16)  64       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_7 (MaxPooling  (None, 7, 50, 50, 16)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 7, 50, 50, 16)     0         \n",
      "                                                                 \n",
      " conv3d_13 (Conv3D)          (None, 7, 50, 50, 32)     13856     \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 7, 50, 50, 32)    128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_8 (MaxPooling  (None, 3, 25, 25, 32)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 3, 25, 25, 32)     0         \n",
      "                                                                 \n",
      " time_distributed_5 (TimeDis  (None, 3, 20000)         0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " gru_9 (GRU)                 (None, 3, 64)             3852672   \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 3, 64)             0         \n",
      "                                                                 \n",
      " gru_10 (GRU)                (None, 3, 128)            74496     \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 3, 128)            0         \n",
      "                                                                 \n",
      " flatten_11 (Flatten)        (None, 384)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 5)                 1925      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,944,453\n",
      "Trainable params: 3,944,357\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv3D, BatchNormalization, TimeDistributed, Flatten, GRU, Dense, Dropout, MaxPooling3D\n",
    "\n",
    "rnn_model8 = Sequential()\n",
    "rnn_model8.add(Conv3D(16, (3, 3, 3), padding='same', activation='relu', input_shape=(num_frames, img_height, img_width, 3)))\n",
    "rnn_model8.add(BatchNormalization())\n",
    "rnn_model8.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "rnn_model8.add(Dropout(0.25))\n",
    "\n",
    "rnn_model8.add(Conv3D(32, (3, 3, 3), padding='same', activation='relu'))\n",
    "rnn_model8.add(BatchNormalization())\n",
    "rnn_model8.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "rnn_model8.add(Dropout(0.25))\n",
    "\n",
    "rnn_model8.add(TimeDistributed(Flatten()))  # Flatten the output before passing to GRU\n",
    "rnn_model8.add(GRU(64, return_sequences=True))\n",
    "rnn_model8.add(Dropout(0.25))\n",
    "rnn_model8.add(GRU(128, return_sequences=True))\n",
    "rnn_model8.add(Dropout(0.25))\n",
    "rnn_model8.add(Flatten())\n",
    "rnn_model8.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "rnn_model8.compile(optimizer=\"sgd\", loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "rnn_model8.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e27d5d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size, img_width, img_height, (40,10))\n",
    "val_generator = generator(val_path, val_doc, batch_size, img_width, img_height,(40,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0ad3a858",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = initializeModel()\n",
    "steps_per_epoch, validation_steps = set_epoch_steps_val_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1d8f15e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /datasets/Project_data/train ; batch size = 32\n",
      "Epoch 1/5\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 1.6082 - categorical_accuracy: 0.235223\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.6056 - categorical_accuracy: 0.2368Source path =  /datasets/Project_data/val ; batch size = 32\n",
      "4\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.55213, saving model to model_init_2024-09-0106_15_55.769024/model-00001-1.60562-0.23680-1.55213-0.32000.h5\n",
      "21/21 [==============================] - 81s 4s/step - loss: 1.6056 - categorical_accuracy: 0.2368 - val_loss: 1.5521 - val_categorical_accuracy: 0.3200 - lr: 0.0100\n",
      "Epoch 2/5\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 1.4790 - categorical_accuracy: 0.368423\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.4704 - categorical_accuracy: 0.37864\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.55213 to 1.49338, saving model to model_init_2024-09-0106_15_55.769024/model-00002-1.47038-0.37858-1.49338-0.38000.h5\n",
      "21/21 [==============================] - 79s 4s/step - loss: 1.4704 - categorical_accuracy: 0.3786 - val_loss: 1.4934 - val_categorical_accuracy: 0.3800 - lr: 0.0100\n",
      "Epoch 3/5\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 1.3336 - categorical_accuracy: 0.486823\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.3345 - categorical_accuracy: 0.48724\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.49338 to 1.38947, saving model to model_init_2024-09-0106_15_55.769024/model-00003-1.33450-0.48718-1.38947-0.50000.h5\n",
      "21/21 [==============================] - 77s 4s/step - loss: 1.3345 - categorical_accuracy: 0.4872 - val_loss: 1.3895 - val_categorical_accuracy: 0.5000 - lr: 0.0100\n",
      "Epoch 4/5\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 1.2341 - categorical_accuracy: 0.509923\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.2244 - categorical_accuracy: 0.51584\n",
      "4\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.38947 to 1.31422, saving model to model_init_2024-09-0106_15_55.769024/model-00004-1.22442-0.51584-1.31422-0.49000.h5\n",
      "21/21 [==============================] - 75s 4s/step - loss: 1.2244 - categorical_accuracy: 0.5158 - val_loss: 1.3142 - val_categorical_accuracy: 0.4900 - lr: 0.0100\n",
      "Epoch 5/5\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 1.0978 - categorical_accuracy: 0.595423\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.0924 - categorical_accuracy: 0.60334\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.31422 to 1.22595, saving model to model_init_2024-09-0106_15_55.769024/model-00005-1.09241-0.60332-1.22595-0.55000.h5\n",
      "21/21 [==============================] - 78s 4s/step - loss: 1.0924 - categorical_accuracy: 0.6033 - val_loss: 1.2259 - val_categorical_accuracy: 0.5500 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f21c0f62280>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model8.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
    "                    callbacks=callbacks_list, validation_data=val_generator,\n",
    "                    validation_steps=validation_steps, class_weight=None, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871f6438",
   "metadata": {},
   "source": [
    "The model is much more balanced now. Training loss and Validation loss are very close to each other. categorical accuracy has dropped but that is to be expected after 5 epochs. Best thing is that validation accuracy and train accuracy are close to each other.Let's reduce the dropout rate to 10% in Conv3D and keep it at 25% in GRU and see if it helps improve the value of accuracy overall while keep at two accuracies close to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519be88c",
   "metadata": {},
   "source": [
    "Experiment 9:\n",
    "1. batch_size = 32\n",
    "2. num_epochs = 5\n",
    "3. img_width, img_height = 100, 100\n",
    "4. Add batch_normalization\n",
    "5. Conv3d + Maxpooling + Dropout(0.10), Conv3d + Maxpooling + Dropout(0.10), TimeDistributed Flatten + GRU + Dropout(0.25) + GRU + Dropout(0.25) + Flatten + Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3073e7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf /home/.local/share/Trash/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "df32f193",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('/datasets/Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('/datasets/Project_data/val.csv').readlines())\n",
    "batch_size = 32 #experiment with the batch size\n",
    "img_idx = [0,2,4,6,8,10,12,14,16,18,20,22,24,26,28]#create a list of image numbers you want to use\n",
    "num_frames = len(img_idx)\n",
    "num_classes = 5\n",
    "batch_size = 32\n",
    "num_epochs = 5\n",
    "img_width = 100\n",
    "img_height = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "47dbd7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_18 (Conv3D)          (None, 15, 100, 100, 16)  1312      \n",
      "                                                                 \n",
      " batch_normalization_18 (Bat  (None, 15, 100, 100, 16)  64       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_13 (MaxPoolin  (None, 7, 50, 50, 16)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 7, 50, 50, 16)     0         \n",
      "                                                                 \n",
      " conv3d_19 (Conv3D)          (None, 7, 50, 50, 32)     13856     \n",
      "                                                                 \n",
      " batch_normalization_19 (Bat  (None, 7, 50, 50, 32)    128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_14 (MaxPoolin  (None, 3, 25, 25, 32)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 3, 25, 25, 32)     0         \n",
      "                                                                 \n",
      " time_distributed_8 (TimeDis  (None, 3, 20000)         0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " gru_15 (GRU)                (None, 3, 64)             3852672   \n",
      "                                                                 \n",
      " dropout_26 (Dropout)        (None, 3, 64)             0         \n",
      "                                                                 \n",
      " gru_16 (GRU)                (None, 3, 128)            74496     \n",
      "                                                                 \n",
      " dropout_27 (Dropout)        (None, 3, 128)            0         \n",
      "                                                                 \n",
      " flatten_17 (Flatten)        (None, 384)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 5)                 1925      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,944,453\n",
      "Trainable params: 3,944,357\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv3D, BatchNormalization, TimeDistributed, Flatten, GRU, Dense, Dropout, MaxPooling3D\n",
    "\n",
    "rnn_model9 = Sequential()\n",
    "rnn_model9.add(Conv3D(16, (3, 3, 3), padding='same', activation='relu', input_shape=(num_frames, img_height, img_width, 3)))\n",
    "rnn_model9.add(BatchNormalization())\n",
    "rnn_model9.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "rnn_model9.add(Dropout(0.10))\n",
    "\n",
    "rnn_model9.add(Conv3D(32, (3, 3, 3), padding='same', activation='relu'))\n",
    "rnn_model9.add(BatchNormalization())\n",
    "rnn_model9.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "rnn_model9.add(Dropout(0.10))\n",
    "\n",
    "rnn_model9.add(TimeDistributed(Flatten()))  # Flatten the output before passing to GRU\n",
    "rnn_model9.add(GRU(64, return_sequences=True))\n",
    "rnn_model9.add(Dropout(0.25))\n",
    "rnn_model9.add(GRU(128, return_sequences=True))\n",
    "rnn_model9.add(Dropout(0.25))\n",
    "rnn_model9.add(Flatten())\n",
    "rnn_model9.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "rnn_model9.compile(optimizer=\"sgd\", loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "rnn_model9.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "30ff2287",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size, img_width, img_height, (40,10))\n",
    "val_generator = generator(val_path, val_doc, batch_size, img_width, img_height,(40,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9ef2b8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = initializeModel()\n",
    "steps_per_epoch, validation_steps = set_epoch_steps_val_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "60e2c926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /datasets/Project_data/train ; batch size = 32\n",
      "Epoch 1/5\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 1.5569 - categorical_accuracy: 0.292823\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.5480 - categorical_accuracy: 0.2926Source path =  /datasets/Project_data/val ; batch size = 32\n",
      "4\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.53994, saving model to model_init_2024-09-0106_15_55.769024/model-00001-1.54805-0.29261-1.53994-0.34000.h5\n",
      "21/21 [==============================] - 82s 4s/step - loss: 1.5480 - categorical_accuracy: 0.2926 - val_loss: 1.5399 - val_categorical_accuracy: 0.3400 - lr: 0.0100\n",
      "Epoch 2/5\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 1.3554 - categorical_accuracy: 0.452323\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.3412 - categorical_accuracy: 0.46764\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.53994 to 1.51097, saving model to model_init_2024-09-0106_15_55.769024/model-00002-1.34116-0.46757-1.51097-0.34000.h5\n",
      "21/21 [==============================] - 79s 4s/step - loss: 1.3412 - categorical_accuracy: 0.4676 - val_loss: 1.5110 - val_categorical_accuracy: 0.3400 - lr: 0.0100\n",
      "Epoch 3/5\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 1.1672 - categorical_accuracy: 0.583923\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.1579 - categorical_accuracy: 0.59134\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.51097 to 1.32868, saving model to model_init_2024-09-0106_15_55.769024/model-00003-1.15791-0.59125-1.32868-0.48000.h5\n",
      "21/21 [==============================] - 79s 4s/step - loss: 1.1579 - categorical_accuracy: 0.5913 - val_loss: 1.3287 - val_categorical_accuracy: 0.4800 - lr: 0.0100\n",
      "Epoch 4/5\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 1.0040 - categorical_accuracy: 0.677623\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9862 - categorical_accuracy: 0.68484\n",
      "4\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.32868 to 1.26473, saving model to model_init_2024-09-0106_15_55.769024/model-00004-0.98617-0.68477-1.26473-0.51000.h5\n",
      "21/21 [==============================] - 76s 4s/step - loss: 0.9862 - categorical_accuracy: 0.6848 - val_loss: 1.2647 - val_categorical_accuracy: 0.5100 - lr: 0.0100\n",
      "Epoch 5/5\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 0.8453 - categorical_accuracy: 0.741823\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8328 - categorical_accuracy: 0.75574\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.26473 to 1.21662, saving model to model_init_2024-09-0106_15_55.769024/model-00005-0.83281-0.75566-1.21662-0.51000.h5\n",
      "21/21 [==============================] - 80s 4s/step - loss: 0.8328 - categorical_accuracy: 0.7557 - val_loss: 1.2166 - val_categorical_accuracy: 0.5100 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f21b7fac790>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model9.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
    "                    callbacks=callbacks_list, validation_data=val_generator,\n",
    "                    validation_steps=validation_steps, class_weight=None, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c3fa28",
   "metadata": {},
   "source": [
    "There is a definite improvement in train accuracy by reducing dropout in the initial layer but validation accuracy is lower than train accuracy by a lot. Let's introduce regularization and see if it helps close the gap between the training and validation accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c630c5",
   "metadata": {},
   "source": [
    "Experiment 10:\n",
    "1. batch_size = 32\n",
    "2. num_epochs = 15\n",
    "3. img_width, img_height = 100, 100\n",
    "4. Add batch_normalization\n",
    "5. Conv3d + Maxpooling + Dropout(0.10) + L2(0.01), Conv3d + Maxpooling + Dropout(0.10) + L2(0.01), TimeDistributed Flatten + GRU + Dropout(0.25)+ L2(0.01) + GRU + Dropout(0.25) + L2(0.01)+ Flatten + Dense + L2(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5b92cdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf /home/.local/share/Trash/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cdfa2c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('/datasets/Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('/datasets/Project_data/val.csv').readlines())\n",
    "batch_size = 32 #experiment with the batch size\n",
    "img_idx = [0,2,4,6,8,10,12,14,16,18,20,22,24,26,28]#create a list of image numbers you want to use\n",
    "num_frames = len(img_idx)\n",
    "num_classes = 5\n",
    "batch_size = 32\n",
    "num_epochs = 15\n",
    "img_width = 100\n",
    "img_height = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0b2a775c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_22 (Conv3D)          (None, 15, 100, 100, 16)  1312      \n",
      "                                                                 \n",
      " batch_normalization_22 (Bat  (None, 15, 100, 100, 16)  64       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_17 (MaxPoolin  (None, 7, 50, 50, 16)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " dropout_32 (Dropout)        (None, 7, 50, 50, 16)     0         \n",
      "                                                                 \n",
      " conv3d_23 (Conv3D)          (None, 7, 50, 50, 32)     13856     \n",
      "                                                                 \n",
      " batch_normalization_23 (Bat  (None, 7, 50, 50, 32)    128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_18 (MaxPoolin  (None, 3, 25, 25, 32)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " dropout_33 (Dropout)        (None, 3, 25, 25, 32)     0         \n",
      "                                                                 \n",
      " time_distributed_10 (TimeDi  (None, 3, 20000)         0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " gru_19 (GRU)                (None, 3, 64)             3852672   \n",
      "                                                                 \n",
      " dropout_34 (Dropout)        (None, 3, 64)             0         \n",
      "                                                                 \n",
      " gru_20 (GRU)                (None, 3, 128)            74496     \n",
      "                                                                 \n",
      " dropout_35 (Dropout)        (None, 3, 128)            0         \n",
      "                                                                 \n",
      " flatten_21 (Flatten)        (None, 384)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 5)                 1925      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,944,453\n",
      "Trainable params: 3,944,357\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv3D, BatchNormalization, TimeDistributed, Flatten, GRU, Dense, Dropout, MaxPooling3D\n",
    "from keras.regularizers import l2\n",
    "\n",
    "rnn_model10 = Sequential()\n",
    "rnn_model10.add(Conv3D(16, (3, 3, 3), padding='same', activation='relu', kernel_regularizer=l2(0.01), input_shape=(num_frames, img_height, img_width, 3)))\n",
    "rnn_model10.add(BatchNormalization())\n",
    "rnn_model10.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "rnn_model10.add(Dropout(0.10))\n",
    "\n",
    "rnn_model10.add(Conv3D(32, (3, 3, 3), padding='same', activation='relu', kernel_regularizer=l2(0.01)))\n",
    "rnn_model10.add(BatchNormalization())\n",
    "rnn_model10.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "rnn_model10.add(Dropout(0.10))\n",
    "\n",
    "rnn_model10.add(TimeDistributed(Flatten()))  # Flatten the output before passing to GRU\n",
    "rnn_model10.add(GRU(64, return_sequences=True, kernel_regularizer=l2(0.01)))\n",
    "rnn_model10.add(Dropout(0.25))\n",
    "rnn_model10.add(GRU(128, return_sequences=True, kernel_regularizer=l2(0.01)))\n",
    "rnn_model10.add(Dropout(0.25))\n",
    "rnn_model10.add(Flatten())\n",
    "rnn_model10.add(Dense(5, activation='softmax', kernel_regularizer=l2(0.01)))\n",
    "\n",
    "# Compile the model\n",
    "rnn_model10.compile(optimizer=\"sgd\", loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "# Now print the model summary\n",
    "rnn_model10.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4e287e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size, img_width, img_height, (40,10))\n",
    "val_generator = generator(val_path, val_doc, batch_size, img_width, img_height,(40,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "70a0a411",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = initializeModel()\n",
    "steps_per_epoch, validation_steps = set_epoch_steps_val_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4674a4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /datasets/Project_data/train ; batch size = 32\n",
      "Epoch 1/15\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 6.8225 - categorical_accuracy: 0.2961 23\n",
      "21/21 [==============================] - ETA: 0s - loss: 6.8128 - categorical_accuracy: 0.3032Source path =  /datasets/Project_data/val ; batch size = 32\n",
      "4\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 6.73724, saving model to model_init_2024-09-0106_15_55.769024/model-00001-6.81278-0.30317-6.73724-0.45000.h5\n",
      "21/21 [==============================] - 83s 4s/step - loss: 6.8128 - categorical_accuracy: 0.3032 - val_loss: 6.7372 - val_categorical_accuracy: 0.4500 - lr: 0.0100\n",
      "Epoch 2/15\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 6.5624 - categorical_accuracy: 0.470423\n",
      "21/21 [==============================] - ETA: 0s - loss: 6.5458 - categorical_accuracy: 0.48114\n",
      "\n",
      "Epoch 00002: val_loss improved from 6.73724 to 6.55454, saving model to model_init_2024-09-0106_15_55.769024/model-00002-6.54584-0.48115-6.55454-0.53000.h5\n",
      "21/21 [==============================] - 79s 4s/step - loss: 6.5458 - categorical_accuracy: 0.4811 - val_loss: 6.5545 - val_categorical_accuracy: 0.5300 - lr: 0.0100\n",
      "Epoch 3/15\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 6.3085 - categorical_accuracy: 0.611823\n",
      "21/21 [==============================] - ETA: 0s - loss: 6.3044 - categorical_accuracy: 0.61844\n",
      "\n",
      "Epoch 00003: val_loss improved from 6.55454 to 6.46800, saving model to model_init_2024-09-0106_15_55.769024/model-00003-6.30435-0.61840-6.46800-0.49000.h5\n",
      "21/21 [==============================] - 80s 4s/step - loss: 6.3044 - categorical_accuracy: 0.6184 - val_loss: 6.4680 - val_categorical_accuracy: 0.4900 - lr: 0.0100\n",
      "Epoch 4/15\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 6.1318 - categorical_accuracy: 0.661223\n",
      "21/21 [==============================] - ETA: 0s - loss: 6.1270 - categorical_accuracy: 0.66374\n",
      "4\n",
      "\n",
      "Epoch 00004: val_loss improved from 6.46800 to 6.29401, saving model to model_init_2024-09-0106_15_55.769024/model-00004-6.12703-0.66365-6.29401-0.56000.h5\n",
      "21/21 [==============================] - 75s 4s/step - loss: 6.1270 - categorical_accuracy: 0.6637 - val_loss: 6.2940 - val_categorical_accuracy: 0.5600 - lr: 0.0100\n",
      "Epoch 5/15\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 5.9386 - categorical_accuracy: 0.728623\n",
      "21/21 [==============================] - ETA: 0s - loss: 5.9263 - categorical_accuracy: 0.73454\n",
      "\n",
      "Epoch 00005: val_loss improved from 6.29401 to 6.18236, saving model to model_init_2024-09-0106_15_55.769024/model-00005-5.92626-0.73454-6.18236-0.53000.h5\n",
      "21/21 [==============================] - 78s 4s/step - loss: 5.9263 - categorical_accuracy: 0.7345 - val_loss: 6.1824 - val_categorical_accuracy: 0.5300 - lr: 0.0100\n",
      "Epoch 6/15\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 5.8072 - categorical_accuracy: 0.774723\n",
      "21/21 [==============================] - ETA: 0s - loss: 5.7952 - categorical_accuracy: 0.77534\n",
      "\n",
      "Epoch 00006: val_loss improved from 6.18236 to 6.06209, saving model to model_init_2024-09-0106_15_55.769024/model-00006-5.79524-0.77526-6.06209-0.70000.h5\n",
      "21/21 [==============================] - 77s 4s/step - loss: 5.7952 - categorical_accuracy: 0.7753 - val_loss: 6.0621 - val_categorical_accuracy: 0.7000 - lr: 0.0100\n",
      "Epoch 7/15\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 5.6792 - categorical_accuracy: 0.786223\n",
      "21/21 [==============================] - ETA: 0s - loss: 5.6720 - categorical_accuracy: 0.79194\n",
      "\n",
      "Epoch 00007: val_loss improved from 6.06209 to 6.03186, saving model to model_init_2024-09-0106_15_55.769024/model-00007-5.67195-0.79186-6.03186-0.60000.h5\n",
      "21/21 [==============================] - 78s 4s/step - loss: 5.6720 - categorical_accuracy: 0.7919 - val_loss: 6.0319 - val_categorical_accuracy: 0.6000 - lr: 0.0100\n",
      "Epoch 8/15\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 5.5189 - categorical_accuracy: 0.850323\n",
      "21/21 [==============================] - ETA: 0s - loss: 5.5171 - categorical_accuracy: 0.85224\n",
      "4\n",
      "\n",
      "Epoch 00008: val_loss improved from 6.03186 to 5.90811, saving model to model_init_2024-09-0106_15_55.769024/model-00008-5.51712-0.85219-5.90811-0.63000.h5\n",
      "21/21 [==============================] - 74s 4s/step - loss: 5.5171 - categorical_accuracy: 0.8522 - val_loss: 5.9081 - val_categorical_accuracy: 0.6300 - lr: 0.0100\n",
      "Epoch 9/15\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 5.3962 - categorical_accuracy: 0.891423\n",
      "21/21 [==============================] - ETA: 0s - loss: 5.3844 - categorical_accuracy: 0.89444\n",
      "\n",
      "Epoch 00009: val_loss improved from 5.90811 to 5.86451, saving model to model_init_2024-09-0106_15_55.769024/model-00009-5.38436-0.89442-5.86451-0.65000.h5\n",
      "21/21 [==============================] - 77s 4s/step - loss: 5.3844 - categorical_accuracy: 0.8944 - val_loss: 5.8645 - val_categorical_accuracy: 0.6500 - lr: 0.0100\n",
      "Epoch 10/15\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 5.2946 - categorical_accuracy: 0.912823\n",
      "21/21 [==============================] - ETA: 0s - loss: 5.2899 - categorical_accuracy: 0.91254\n",
      "\n",
      "Epoch 00010: val_loss improved from 5.86451 to 5.77852, saving model to model_init_2024-09-0106_15_55.769024/model-00010-5.28993-0.91252-5.77852-0.63000.h5\n",
      "21/21 [==============================] - 78s 4s/step - loss: 5.2899 - categorical_accuracy: 0.9125 - val_loss: 5.7785 - val_categorical_accuracy: 0.6300 - lr: 0.0100\n",
      "Epoch 11/15\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 5.1696 - categorical_accuracy: 0.942423\n",
      "21/21 [==============================] - ETA: 0s - loss: 5.1670 - categorical_accuracy: 0.94424\n",
      "\n",
      "Epoch 00011: val_loss improved from 5.77852 to 5.74660, saving model to model_init_2024-09-0106_15_55.769024/model-00011-5.16699-0.94419-5.74660-0.64000.h5\n",
      "21/21 [==============================] - 78s 4s/step - loss: 5.1670 - categorical_accuracy: 0.9442 - val_loss: 5.7466 - val_categorical_accuracy: 0.6400 - lr: 0.0100\n",
      "Epoch 12/15\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 5.0699 - categorical_accuracy: 0.9671 23\n",
      "21/21 [==============================] - ETA: 0s - loss: 5.0729 - categorical_accuracy: 0.96384\n",
      "4\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 5.74660\n",
      "21/21 [==============================] - 80s 4s/step - loss: 5.0729 - categorical_accuracy: 0.9638 - val_loss: 5.7823 - val_categorical_accuracy: 0.5900 - lr: 0.0100\n",
      "Epoch 13/15\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 5.0001 - categorical_accuracy: 0.975323\n",
      "21/21 [==============================] - ETA: 0s - loss: 4.9923 - categorical_accuracy: 0.97744\n",
      "\n",
      "Epoch 00013: val_loss improved from 5.74660 to 5.65519, saving model to model_init_2024-09-0106_15_55.769024/model-00013-4.99226-0.97738-5.65519-0.65000.h5\n",
      "21/21 [==============================] - 81s 4s/step - loss: 4.9923 - categorical_accuracy: 0.9774 - val_loss: 5.6552 - val_categorical_accuracy: 0.6500 - lr: 0.0100\n",
      "Epoch 14/15\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 4.9209 - categorical_accuracy: 0.990123\n",
      "21/21 [==============================] - ETA: 0s - loss: 4.9141 - categorical_accuracy: 0.99104\n",
      "\n",
      "Epoch 00014: val_loss improved from 5.65519 to 5.54131, saving model to model_init_2024-09-0106_15_55.769024/model-00014-4.91413-0.99095-5.54131-0.67000.h5\n",
      "21/21 [==============================] - 79s 4s/step - loss: 4.9141 - categorical_accuracy: 0.9910 - val_loss: 5.5413 - val_categorical_accuracy: 0.6700 - lr: 0.0100\n",
      "Epoch 15/15\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 4.8494 - categorical_accuracy: 0.993423\n",
      "21/21 [==============================] - ETA: 0s - loss: 4.8471 - categorical_accuracy: 0.99404\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 5.54131\n",
      "21/21 [==============================] - 78s 4s/step - loss: 4.8471 - categorical_accuracy: 0.9940 - val_loss: 5.5623 - val_categorical_accuracy: 0.7000 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f21c13d58e0>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model10.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
    "                    callbacks=callbacks_list, validation_data=val_generator,\n",
    "                    validation_steps=validation_steps, class_weight=None, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68212d3",
   "metadata": {},
   "source": [
    "The training accuracy is really good at 99.4% but validation accuracy is 70%. Let's increase dropout to 25% along with regularization and see if it closes the gap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cd6643",
   "metadata": {},
   "source": [
    "Experiment 11:\n",
    "\n",
    "batch_size = 32\n",
    "num_epochs = 15\n",
    "img_width, img_height = 100, 100\n",
    "Add batch_normalization\n",
    "Conv3d + Maxpooling + Dropout(0.25) + L2(0.01), Conv3d + Maxpooling + Dropout(0.25) + L2(0.01), TimeDistributed Flatten + GRU + Dropout(0.25)+ L2(0.01) + GRU + Dropout(0.25) + L2(0.01)+ Flatten + Dense + L2(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c6469bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf /home/.local/share/Trash/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0c171227",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('/datasets/Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('/datasets/Project_data/val.csv').readlines())\n",
    "batch_size = 32 #experiment with the batch size\n",
    "img_idx = [0,2,4,6,8,10,12,14,16,18,20,22,24,26,28]#create a list of image numbers you want to use\n",
    "num_frames = len(img_idx)\n",
    "num_classes = 5\n",
    "batch_size = 32\n",
    "num_epochs = 15\n",
    "img_width = 100\n",
    "img_height = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d89ddcd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_26 (Conv3D)          (None, 15, 100, 100, 16)  1312      \n",
      "                                                                 \n",
      " batch_normalization_26 (Bat  (None, 15, 100, 100, 16)  64       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_21 (MaxPoolin  (None, 7, 50, 50, 16)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " dropout_40 (Dropout)        (None, 7, 50, 50, 16)     0         \n",
      "                                                                 \n",
      " conv3d_27 (Conv3D)          (None, 7, 50, 50, 32)     13856     \n",
      "                                                                 \n",
      " batch_normalization_27 (Bat  (None, 7, 50, 50, 32)    128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_22 (MaxPoolin  (None, 3, 25, 25, 32)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " dropout_41 (Dropout)        (None, 3, 25, 25, 32)     0         \n",
      "                                                                 \n",
      " time_distributed_12 (TimeDi  (None, 3, 20000)         0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " gru_23 (GRU)                (None, 3, 64)             3852672   \n",
      "                                                                 \n",
      " dropout_42 (Dropout)        (None, 3, 64)             0         \n",
      "                                                                 \n",
      " gru_24 (GRU)                (None, 3, 128)            74496     \n",
      "                                                                 \n",
      " dropout_43 (Dropout)        (None, 3, 128)            0         \n",
      "                                                                 \n",
      " flatten_25 (Flatten)        (None, 384)               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 5)                 1925      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,944,453\n",
      "Trainable params: 3,944,357\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv3D, BatchNormalization, TimeDistributed, Flatten, GRU, Dense, Dropout, MaxPooling3D\n",
    "from keras.regularizers import l2\n",
    "\n",
    "rnn_model11 = Sequential()\n",
    "rnn_model11.add(Conv3D(16, (3, 3, 3), padding='same', activation='relu', kernel_regularizer=l2(0.01), input_shape=(num_frames, img_height, img_width, 3)))\n",
    "rnn_model11.add(BatchNormalization())\n",
    "rnn_model11.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "rnn_model11.add(Dropout(0.25))\n",
    "\n",
    "rnn_model11.add(Conv3D(32, (3, 3, 3), padding='same', activation='relu', kernel_regularizer=l2(0.01)))\n",
    "rnn_model11.add(BatchNormalization())\n",
    "rnn_model11.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "rnn_model11.add(Dropout(0.25))\n",
    "\n",
    "rnn_model11.add(TimeDistributed(Flatten()))  # Flatten the output before passing to GRU\n",
    "rnn_model11.add(GRU(64, return_sequences=True, kernel_regularizer=l2(0.01)))\n",
    "rnn_model11.add(Dropout(0.25))\n",
    "rnn_model11.add(GRU(128, return_sequences=True, kernel_regularizer=l2(0.01)))\n",
    "rnn_model11.add(Dropout(0.25))\n",
    "rnn_model11.add(Flatten())\n",
    "rnn_model11.add(Dense(5, activation='softmax', kernel_regularizer=l2(0.01)))\n",
    "\n",
    "# Compile the model\n",
    "rnn_model11.compile(optimizer=\"sgd\", loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "# Now print the model summary\n",
    "rnn_model11.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7ab4a819",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size, img_width, img_height, (40,10))\n",
    "val_generator = generator(val_path, val_doc, batch_size, img_width, img_height,(40,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a0b39f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = initializeModel()\n",
    "steps_per_epoch, validation_steps = set_epoch_steps_val_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c7a95805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /datasets/Project_data/train ; batch size = 32\n",
      "Epoch 1/15\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 6.8538 - categorical_accuracy: 0.235223\n",
      "21/21 [==============================] - ETA: 0s - loss: 6.8400 - categorical_accuracy: 0.2504Source path =  /datasets/Project_data/val ; batch size = 32\n",
      "4\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 6.76166, saving model to model_init_2024-09-0106_15_55.769024/model-00001-6.84000-0.25038-6.76166-0.33000.h5\n",
      "21/21 [==============================] - 81s 4s/step - loss: 6.8400 - categorical_accuracy: 0.2504 - val_loss: 6.7617 - val_categorical_accuracy: 0.3300 - lr: 0.0100\n",
      "Epoch 2/15\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 6.6693 - categorical_accuracy: 0.388223\n",
      "21/21 [==============================] - ETA: 0s - loss: 6.6742 - categorical_accuracy: 0.38764\n",
      "\n",
      "Epoch 00002: val_loss improved from 6.76166 to 6.64800, saving model to model_init_2024-09-0106_15_55.769024/model-00002-6.67419-0.38763-6.64800-0.40000.h5\n",
      "21/21 [==============================] - 78s 4s/step - loss: 6.6742 - categorical_accuracy: 0.3876 - val_loss: 6.6480 - val_categorical_accuracy: 0.4000 - lr: 0.0100\n",
      "Epoch 3/15\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 6.5299 - categorical_accuracy: 0.457223\n",
      "21/21 [==============================] - ETA: 0s - loss: 6.5256 - categorical_accuracy: 0.46764\n",
      "\n",
      "Epoch 00003: val_loss improved from 6.64800 to 6.54471, saving model to model_init_2024-09-0106_15_55.769024/model-00003-6.52562-0.46757-6.54471-0.47000.h5\n",
      "21/21 [==============================] - 77s 4s/step - loss: 6.5256 - categorical_accuracy: 0.4676 - val_loss: 6.5447 - val_categorical_accuracy: 0.4700 - lr: 0.0100\n",
      "Epoch 4/15\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 6.3938 - categorical_accuracy: 0.518123\n",
      "21/21 [==============================] - ETA: 0s - loss: 6.3797 - categorical_accuracy: 0.52344\n",
      "4\n",
      "\n",
      "Epoch 00004: val_loss improved from 6.54471 to 6.43283, saving model to model_init_2024-09-0106_15_55.769024/model-00004-6.37972-0.52338-6.43283-0.50000.h5\n",
      "21/21 [==============================] - 73s 4s/step - loss: 6.3797 - categorical_accuracy: 0.5234 - val_loss: 6.4328 - val_categorical_accuracy: 0.5000 - lr: 0.0100\n",
      "Epoch 5/15\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 6.2084 - categorical_accuracy: 0.606923\n",
      "21/21 [==============================] - ETA: 0s - loss: 6.2069 - categorical_accuracy: 0.60634\n",
      "\n",
      "Epoch 00005: val_loss improved from 6.43283 to 6.31659, saving model to model_init_2024-09-0106_15_55.769024/model-00005-6.20689-0.60633-6.31659-0.53000.h5\n",
      "21/21 [==============================] - 78s 4s/step - loss: 6.2069 - categorical_accuracy: 0.6063 - val_loss: 6.3166 - val_categorical_accuracy: 0.5300 - lr: 0.0100\n",
      "Epoch 6/15\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 6.0386 - categorical_accuracy: 0.671123\n",
      "21/21 [==============================] - ETA: 0s - loss: 6.0380 - categorical_accuracy: 0.67274\n",
      "\n",
      "Epoch 00006: val_loss improved from 6.31659 to 6.16962, saving model to model_init_2024-09-0106_15_55.769024/model-00006-6.03797-0.67270-6.16962-0.59000.h5\n",
      "21/21 [==============================] - 77s 4s/step - loss: 6.0380 - categorical_accuracy: 0.6727 - val_loss: 6.1696 - val_categorical_accuracy: 0.5900 - lr: 0.0100\n",
      "Epoch 7/15\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 5.8850 - categorical_accuracy: 0.736823\n",
      "21/21 [==============================] - ETA: 0s - loss: 5.8732 - categorical_accuracy: 0.74214\n",
      "\n",
      "Epoch 00007: val_loss improved from 6.16962 to 6.05043, saving model to model_init_2024-09-0106_15_55.769024/model-00007-5.87321-0.74208-6.05043-0.57000.h5\n",
      "21/21 [==============================] - 78s 4s/step - loss: 5.8732 - categorical_accuracy: 0.7421 - val_loss: 6.0504 - val_categorical_accuracy: 0.5700 - lr: 0.0100\n",
      "Epoch 8/15\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 5.7235 - categorical_accuracy: 0.791123\n",
      "21/21 [==============================] - ETA: 0s - loss: 5.7160 - categorical_accuracy: 0.79344\n",
      "4\n",
      "\n",
      "Epoch 00008: val_loss improved from 6.05043 to 5.98578, saving model to model_init_2024-09-0106_15_55.769024/model-00008-5.71604-0.79336-5.98578-0.58000.h5\n",
      "21/21 [==============================] - 74s 4s/step - loss: 5.7160 - categorical_accuracy: 0.7934 - val_loss: 5.9858 - val_categorical_accuracy: 0.5800 - lr: 0.0100\n",
      "Epoch 9/15\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 5.6032 - categorical_accuracy: 0.817423\n",
      "21/21 [==============================] - ETA: 0s - loss: 5.5965 - categorical_accuracy: 0.81904\n",
      "\n",
      "Epoch 00009: val_loss improved from 5.98578 to 5.91840, saving model to model_init_2024-09-0106_15_55.769024/model-00009-5.59646-0.81900-5.91840-0.60000.h5\n",
      "21/21 [==============================] - 75s 4s/step - loss: 5.5965 - categorical_accuracy: 0.8190 - val_loss: 5.9184 - val_categorical_accuracy: 0.6000 - lr: 0.0100\n",
      "Epoch 10/15\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 5.4621 - categorical_accuracy: 0.866823\n",
      "21/21 [==============================] - ETA: 0s - loss: 5.4491 - categorical_accuracy: 0.87634\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 5.91840\n",
      "21/21 [==============================] - 76s 4s/step - loss: 5.4491 - categorical_accuracy: 0.8763 - val_loss: 5.9223 - val_categorical_accuracy: 0.5100 - lr: 0.0100\n",
      "Epoch 11/15\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 5.3916 - categorical_accuracy: 0.848723\n",
      "21/21 [==============================] - ETA: 0s - loss: 5.3864 - categorical_accuracy: 0.85074\n",
      "\n",
      "Epoch 00011: val_loss improved from 5.91840 to 5.79898, saving model to model_init_2024-09-0106_15_55.769024/model-00011-5.38641-0.85068-5.79898-0.57000.h5\n",
      "21/21 [==============================] - 76s 4s/step - loss: 5.3864 - categorical_accuracy: 0.8507 - val_loss: 5.7990 - val_categorical_accuracy: 0.5700 - lr: 0.0100\n",
      "Epoch 12/15\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 5.2585 - categorical_accuracy: 0.888223\n",
      "21/21 [==============================] - ETA: 0s - loss: 5.2506 - categorical_accuracy: 0.89144\n",
      "4\n",
      "\n",
      "Epoch 00012: val_loss improved from 5.79898 to 5.73491, saving model to model_init_2024-09-0106_15_55.769024/model-00012-5.25055-0.89140-5.73491-0.65000.h5\n",
      "21/21 [==============================] - 73s 4s/step - loss: 5.2506 - categorical_accuracy: 0.8914 - val_loss: 5.7349 - val_categorical_accuracy: 0.6500 - lr: 0.0100\n",
      "Epoch 13/15\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 5.1693 - categorical_accuracy: 0.929323\n",
      "21/21 [==============================] - ETA: 0s - loss: 5.1690 - categorical_accuracy: 0.92614\n",
      "\n",
      "Epoch 00013: val_loss improved from 5.73491 to 5.73160, saving model to model_init_2024-09-0106_15_55.769024/model-00013-5.16898-0.92609-5.73160-0.56000.h5\n",
      "21/21 [==============================] - 76s 4s/step - loss: 5.1690 - categorical_accuracy: 0.9261 - val_loss: 5.7316 - val_categorical_accuracy: 0.5600 - lr: 0.0100\n",
      "Epoch 14/15\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 5.0718 - categorical_accuracy: 0.937523\n",
      "21/21 [==============================] - ETA: 0s - loss: 5.0681 - categorical_accuracy: 0.93824\n",
      "\n",
      "Epoch 00014: val_loss improved from 5.73160 to 5.63079, saving model to model_init_2024-09-0106_15_55.769024/model-00014-5.06807-0.93816-5.63079-0.62000.h5\n",
      "21/21 [==============================] - 75s 4s/step - loss: 5.0681 - categorical_accuracy: 0.9382 - val_loss: 5.6308 - val_categorical_accuracy: 0.6200 - lr: 0.0100\n",
      "Epoch 15/15\n",
      "19/21 [==========================>...] - ETA: 6s - loss: 4.9773 - categorical_accuracy: 0.962223\n",
      "21/21 [==============================] - ETA: 0s - loss: 4.9658 - categorical_accuracy: 0.96534\n",
      "\n",
      "Epoch 00015: val_loss improved from 5.63079 to 5.58986, saving model to model_init_2024-09-0106_15_55.769024/model-00015-4.96584-0.96531-5.58986-0.67000.h5\n",
      "21/21 [==============================] - 77s 4s/step - loss: 4.9658 - categorical_accuracy: 0.9653 - val_loss: 5.5899 - val_categorical_accuracy: 0.6700 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f21c1363f70>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model11.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
    "                    callbacks=callbacks_list, validation_data=val_generator,\n",
    "                    validation_steps=validation_steps, class_weight=None, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad220bc3",
   "metadata": {},
   "source": [
    "FINAL CONCLUSION:\n",
    "We will go with model 10, it had better train accuracy anf "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
